{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written By:- Aadish Joshi  \n",
    "Date:- April 24, 2019  \n",
    "Topic:- Language Modeling with Ngrams  \n",
    "\n",
    "➢ What are we trying to solve here?  \n",
    "Using probabilistic models called N-grams to predict the next word from the previous n-1 words.  \n",
    "Note that this problem is corpus (data set of the sentences) specific.  \n",
    "\n",
    "➢ Grams refered to number of words taken into consideration. e.g. unigram means occurence of the single word in the corpus. Bigrams means predicting the next word based on 1 previous word. Trigrams mean predicting the next word based on 2 previous words and so on.\n",
    "\n",
    "➢This method is count based. A simple approach uses count.  \n",
    "Count all the unigrams. e.g. count of all the words in the corpus. \n",
    "\n",
    "➢ We’ll call a statistical model that can assess this probability a Language Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➢ Consider following corpus  \n",
    "\n",
    "\"The lake is very big. Its water is so transparent that you can see your face clearly. Its water is so transparent that the moon appears even bigger due to reflection\"  \n",
    "\n",
    "How to estimate the probability of the word \"the\" given previous words \"its water is so transparent that\"  \n",
    "\n",
    "We calculate    \n",
    "\n",
    "Count(its water is so transparent that the) = 1\n",
    "and  \n",
    "Count(its water is so transparent that) = 2\n",
    "\n",
    "Hence  \n",
    "\n",
    "P(the | its water is so transparent that ) = Count(its water is so transparent that the) / Count(its water is so transparent that)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, for most sequences and for most text collections we won’t get good estimates from this method.  \n",
    "➢ What we’re likely to get is 0. Or worse 0/0.  \n",
    "\n",
    "➢ Let’s use the chain rule of probability  \n",
    "➢And a particularly useful independence assumption.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain rule of Probability.\n",
    "P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)  \n",
    "\n",
    "Independence Assumption.(Markov Assumption)  \n",
    "That is, the probability in question is independent of its earlier history."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
